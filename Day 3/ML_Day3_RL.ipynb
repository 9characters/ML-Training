{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Frozen Lake Q Learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlDx8SvFihOz"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import time\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXMCHSo-i0tK"
      },
      "source": [
        "#Make the environment\n",
        "env = gym.make(\"FrozenLake-v0\")\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xxeO6O0jrsv"
      },
      "source": [
        "action_space_size = env.action_space.n\n",
        "state_space_size = env.observation_space.n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKa44mqLja5O"
      },
      "source": [
        "#Build the Q-Table\n",
        "q_table = np.zeros((state_space_size, action_space_size))\n",
        "print(q_table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpqegKUjkYL-"
      },
      "source": [
        "#Create and Initialize all the parameters needed to implement the Q-Learning algorithm\n",
        "num_episodes = 10000\n",
        "max_steps_per_episode = 100 #For termination, in a episode\n",
        "\n",
        "learning_rate = 0.1\n",
        "discount_rate = 0.99 # Between 0 and 1 symbol\n",
        "\n",
        "exploration_rate = 1 # epsilon value\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.001 #Change with experiment\n",
        "\n",
        "#The above parameters are to be trained and tuned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WUGzS1elo5_"
      },
      "source": [
        "#Create a list to store the rewards in each episode\n",
        "#To see how the game scores change everytime\n",
        "rewards_all_episodes = []\n",
        "\n",
        "# Q-learning algorithm\n",
        "for episode in range(num_episodes):\n",
        "    #Resetting to starting state for each episode\n",
        "    state = env.reset()\n",
        "    #To make sure the episode is finished\n",
        "    done = False \n",
        "    rewards_current_episode = 0 #Keep track of reward in each episode\n",
        "\n",
        "    for step in range(max_steps_per_episode): \n",
        "        # Exploration-exploitation trade-off\n",
        "        exploration_rate_threshold = random.uniform(0, 1) #This is needed to whether know that our agent will eplore\n",
        "                                                          # or exploit the environment\n",
        "        if exploration_rate_threshold > exploration_rate: \n",
        "            # exploitation\n",
        "            action = np.argmax(q_table[state,:])\n",
        "        else:\n",
        "            # Exploration\n",
        "            action = env.action_space.sample()      \n",
        "\n",
        "        # After the action is chosen, we then execute the action\n",
        "        new_state, reward, done, info = env.step(action) \n",
        "        \n",
        "        #1. new_state \n",
        "        #2. reward for the action \n",
        "        #3. whether or not action ended our episode \n",
        "        #4. diagnostic information of the environment\n",
        "\n",
        "        # Update Q-table for Q(s,a) as per the formula (weighted sum of old value and learned value)\n",
        "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
        "                                learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
        "\n",
        "        state = new_state #Current state is the new state\n",
        "        rewards_current_episode += reward #add the cumulative reward\n",
        "\n",
        "        #check if the episode is over or not\n",
        "        if done == True: \n",
        "          break\n",
        "\n",
        "    # Exploration rate decay\n",
        "    exploration_rate = min_exploration_rate + \\\n",
        "                       (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
        "                       \n",
        "    #appends the rewards from the current episode               \n",
        "    rewards_all_episodes.append(rewards_current_episode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6BIBxbPrmIG"
      },
      "source": [
        "# After all episodes are completed, we calculate average reward per 1000 episodes\n",
        "# Calculate and print the average reward per thousand episodes\n",
        "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
        "count = 1000\n",
        "\n",
        "print(\"********Average reward per thousand episodes********\\n\")\n",
        "for r in rewards_per_thousand_episodes:\n",
        "    print(count, \": \", str(sum(r/1000)))\n",
        "    count += 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYlQp2Rqr7zo"
      },
      "source": [
        "# Print updated Q-table\n",
        "print(\"\\n\\n********Q-table********\\n\")\n",
        "print(q_table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx8rZJ_Ys2Xe"
      },
      "source": [
        "for episode in range(3):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
        "    time.sleep(1)\n",
        "\n",
        "    for step in range(max_steps_per_episode):        \n",
        "        clear_output(wait=True)\n",
        "        env.render() # render the current state of the environment to the display so \n",
        "                     # that we can visualize the gamegrid and agent gale play\n",
        "        time.sleep(0.3)\n",
        "\n",
        "        action = np.argmax(q_table[state,:])        \n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # If the episode is completed then we render the environment to see where the agent ended in the\n",
        "        # environment by taking that action\n",
        "        if done:\n",
        "            clear_output(wait=True)\n",
        "            env.render()\n",
        "            if reward == 1:\n",
        "                print(\"****You reached the goal!****\")\n",
        "                time.sleep(3)\n",
        "            else:\n",
        "                print(\"****You fell through a hole!****\")\n",
        "                time.sleep(3)\n",
        "            clear_output(wait=True)\n",
        "            break\n",
        "        \n",
        "        state = new_state\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}